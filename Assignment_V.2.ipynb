{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zedin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer , PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/dataset.csv does not exist: 'data/dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eb4f77b80bb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/dataset.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"spam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File data/dataset.csv does not exist: 'data/dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", names = [\"spam\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"\\nNo. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('spam').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df.spam.value_counts()\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Spam', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['text'].map(lambda text: len(text))\n",
    "\n",
    "df.groupby('spam').length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_subset = df[df.length < 1800]\n",
    "emails_subset.hist(column='length', by='spam', bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.spam.replace(('ham', 'spam'), (0, 1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Lemmatization\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['text_lemmatized'] = df['text'].map(lambda text: ' '.join(lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['text_steam'] = df['text'].map(lambda text: ' '.join(stemmer.stem(w) for w in nltk.word_tokenize(text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ''.join(list(df[df['spam']==1]['text_lemmatized']))\n",
    "spam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wordclod)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ''.join(list(df[df['spam']==0]['text_lemmatized']))\n",
    "spam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wordclod)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ''.join(list(df[df['spam']==1]['text_steam']))\n",
    "spam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wordclod)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ''.join(list(df[df['spam']==0]['text_steam']))\n",
    "spam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wordclod)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text_lemmatized\"]\n",
    "y = df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_multinom = CountVectorizer(lowercase=True, stop_words='english',binary = False)\n",
    "count_vect_multivar = CountVectorizer(lowercase=True, stop_words='english',binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_nom = count_vect_multinom.fit_transform(X)\n",
    "X_counts_var = count_vect_multivar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X_nom = np.array(X_counts_nom.toarray())\n",
    "X_var = np.array(X_counts_var.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Multivariate_NB:\n",
    "    def __init_(self,alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "        self.pie_1 = None\n",
    "        self.pie_0 = None\n",
    "        self.theta_jc_1 = None\n",
    "        self.theta_jc_0 = None\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        pie_num = (Y == 1).astype(int).sum() + 1 \n",
    "        pie_denum = len(set(Y)) + len(Y)\n",
    "        self.pie_0 = pie_num / pie_denum       \n",
    "        self.pie_1 = 1 - self.pie_0\n",
    "        \n",
    "        \n",
    "        N_jc_0 =  X[Y == 0].sum(axis=0) \n",
    "        N_c_0 = X[Y==0].shape[0]\n",
    "        self.theta_jc_0 = (N_jc_0 + 1) / (2 + N_c_0)\n",
    "        \n",
    "        N_jc_1 =  X[Y == 1].sum(axis=0) \n",
    "        N_c_1 = X[Y==1].shape[0]\n",
    "        self.theta_jc_1 = (N_jc_1 + 1) / (2 + N_c_1)                   \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "        \n",
    "    def predict_log_proba(self, X):\n",
    "        a = self.theta_jc_1[:,None]\n",
    "        b = self.theta_jc_0[:,None]\n",
    "        log_prob_1 = np.log(self.pie_1) + np.log(np.where(X.T*a != 0,a, 1-a )).sum(axis=0).T\n",
    "        log_prob_1 = log_prob_1[:,None]\n",
    "        log_prob_0 = np.log(self.pie_0) + np.log(np.where(X.T*b != 0,b, 1-b )).sum(axis=0).T\n",
    "        log_prob_0 = log_prob_0[:,None]\n",
    "        return np.concatenate((log_prob_0,log_prob_1),axis = 1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Multinomial_NB:\n",
    "    def __init__(self,alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self,X,Y,alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        #self.pie_1 = (Y.sum() + 1 ) / (len(set(Y)) + len(Y))\n",
    "        self.pie_1 = (Y.sum() + self.alpha ) / (len(set(Y))*self.alpha + len(Y))\n",
    "\n",
    "        self.theta_jc = np.zeros((2, X.shape[1]))\n",
    "        ham_doc = X[Y == 0]\n",
    "        #self.theta_jc[0] = (ham_doc.sum(axis=0) + 1) / (np.einsum('ij->',ham_doc) + X.shape[1])\n",
    "        self.theta_jc[0] = (ham_doc.sum(axis=0) + self.alpha) / (np.einsum('ij->',ham_doc) + X.shape[1]*self.alpha)\n",
    "\n",
    "        spam_doc = X[Y == 1]\n",
    "        #self.theta_jc[1] = (spam_doc.sum(axis=0)+1) / (np.einsum('ij->',spam_doc)+X.shape[1])\n",
    "        self.theta_jc[1] = (spam_doc.sum(axis=0)+self.alpha) / (np.einsum('ij->',spam_doc)+X.shape[1]*self.alpha)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return np.sum(X[:,None] * np.log(self.theta_jc), axis=-1) + np.log([1-self.pie_1,self.pie_1])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(Y_true, Y_pred):\n",
    "    E = np.array(Y_true).reshape(-1,1) - np.array(Y_pred).reshape(-1,1)\n",
    "    mse = 1/np.array(Y_true).shape[0] * (E.T.dot(E))\n",
    "    return mse[(0,0)]\n",
    "\n",
    "def accuracy(x,y):\n",
    "    x,y = np.array(x),np.array(y)\n",
    "    pred = (x == y).astype(np.int)\n",
    "    return pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainTest(X,y,t):\n",
    "    train_size = int((1-t) * X.shape[0])   \n",
    "    return X[:train_size],X[train_size:],y[:train_size],y[train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_trainTest(X_var,y,t=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multivariate_NB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pre = model.predict(X_test)\n",
    "mse(y_test, y_pre)\n",
    "accuracy(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pre_sk = clf.predict(X_test)\n",
    "print(y_pre_sk)\n",
    "mse(y_test, y_pre_sk)\n",
    "accuracy(y_test, y_pre_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_trainTest(X_nom,y,t=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multinomial_NB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pre = model.predict(X_test)\n",
    "y_pre\n",
    "mse(y_test, y_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pre_sk = clf.predict(X_test)\n",
    "y_pre_sk\n",
    "mse(y_test, y_pre_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(folds,data,labels,model,error_fuction,**model_args):\n",
    "#def sFold(folds,data,labels,model,error_fuction,**model_args):\n",
    "    if(labels.shape == (labels.shape[0],)):\n",
    "        labels = np.expand_dims(labels,axis=1)\n",
    "    dataset = np.concatenate([data,labels],axis=1)\n",
    "    s_part = s_partition(dataset,folds)\n",
    "    pred_y = []\n",
    "    true_y = []\n",
    "    for idx,val in enumerate(s_part):\n",
    "        test_y = val[:,-1]\n",
    "        #test_y = np.expand_dims(test_y, axis=1)\n",
    "        test = val[:,:-1]\n",
    "        train = np.concatenate(np.delete(s_part,idx,0))\n",
    "        label = train[:,-1]\n",
    "        train = train[:,:-1]        \n",
    "        model.fit(train,label,**model_args)       \n",
    "        pred = model.predict(test)\n",
    "        pred_y.append(pred)\n",
    "        true_y.append(test_y)\n",
    "    pred_y = np.concatenate(pred_y)\n",
    "    true_y = np.concatenate(true_y)\n",
    "\n",
    "    avg_error = error_fuction(pred_y,true_y).mean()   \n",
    "    result = {'Expected labels':true_y, 'Predicted labels': pred_y,'Average error':avg_error }\n",
    "    return result\n",
    "\n",
    "\n",
    "#helper\n",
    "def s_partition(x,s):\n",
    "    return np.array_split(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = np.empty((4,2,2))\n",
    "validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(actual, predicted):\n",
    "    \n",
    "    CM =  compute_confusion_matrix(actual, predicted).to_numpy()  # CM is converted into a 2 X 2 array.\n",
    "    \n",
    "    TN = CM[0,0]; FP = CM[0,1]; FN = CM[1,0]; TP =  CM[1,1];\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def compute_confusion_matrix(actual, predicted):\n",
    "    \n",
    "    arary_actual = np.array(actual)\n",
    "    array_pred = np.array(predicted)\n",
    "    \n",
    "    pd_actual = pd.Series(arary_actual, name='Actual')\n",
    "    pd_predicted = pd.Series(array_pred, name='Predicted')\n",
    "\n",
    "    pd_actual = pd.Categorical(pd_actual, categories=[0, 1])\n",
    "    pd_predicted = pd.Categorical(pd_predicted, categories=[0, 1])\n",
    "\n",
    "    CM =  pd.crosstab(pd_actual, pd_predicted, dropna=False)\n",
    "    \n",
    "    return CM\n",
    "\n",
    "def compute_precision(actual, predicted):\n",
    "       \n",
    "    CM =  compute_confusion_matrix(actual, predicted).to_numpy()  # CM is converted into a 2 X 2 array.\n",
    "    \n",
    "    TN = CM[0,0]; FP = CM[0,1]; FN = CM[1,0]; TP =  CM[1,1];\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_F1_score(actual, predicted):\n",
    "    \n",
    "    precision = compute_precision(actual, predicted)\n",
    "    recall = compute_recall(actual, predicted)\n",
    "    \n",
    "    F1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = np.empty((8,1))\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "maxScore = 0\n",
    "for i,k in enumerate(alpha):\n",
    "            multi_nom = Multinomial_NB()\n",
    "            model_args = {'alpha' : k}\n",
    "            result = sFold(5,X_train,y_train,multi_nom,compute_F1_score,**model_args)\n",
    "            validation_accuracy[i] = result['Average error']\n",
    "            if validation_accuracy[i] > maxScore:\n",
    "                maxScore = validation_accuracy[i]\n",
    "                index = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal alpha: ',alpha[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multinomial_NB()\n",
    "model.fit(X_train,y_train,alpha = 2)\n",
    "y_pre = model.predict(X_test)\n",
    "y_pre\n",
    "compute_F1_score(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing on Test DATA')\n",
    "print('Precision :',compute_precision(y_test, y_pre))\n",
    "print('Recall :',compute_recall(y_test, y_pre))\n",
    "print('F1 Score :',compute_F1_score(y_test, y_pre))\n",
    "print('\\n\\nConfusion Matrix :\\n')\n",
    "print(compute_confusion_matrix(y_test, y_pre))\n",
    "print('\\n\\nAccuracy :',accuracy(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ROC_elements(y_label, y_prob, target_label = 1):\n",
    "    \n",
    "    # gets the target label.\n",
    "    if target_label == 0: non_target_label = 1\n",
    "    if target_label == 1: non_target_label = 0\n",
    "    \n",
    "    # converts the input arguments into arrays.\n",
    "    ar_y_label = np.array(y_label)\n",
    "    ar_y_prob = np.array(y_prob)\n",
    "    \n",
    "    # creates a list to sort the results of predicted y. \n",
    "    y_pred = list(y_prob)\n",
    "    \n",
    "    # generates list to store the tpr, fpr and threshold.\n",
    "    tpr_list = [0, 1]     \n",
    "    fpr_list = [0, 1]\n",
    "    thres_lish = [1, 0]\n",
    "    \n",
    "    # using the for loop to predicte y based on the input y_prob. \n",
    "    for i, prob in enumerate(ar_y_prob):\n",
    "        threshold = prob\n",
    "        for index, y_prob in enumerate(ar_y_prob):\n",
    "            if y_prob >= threshold:\n",
    "                y_pred[index] = target_label\n",
    "            else:\n",
    "                y_pred[index] = non_target_label\n",
    "        \n",
    "        # uses the function to compute the confusion matrix, and gets the TN, FP, FN, TP. \n",
    "        CM = compute_confusion_matrix(y_label, y_pred).to_numpy()           \n",
    "        TN = CM[0,0]; FP = CM[0,1]; FN = CM[1,0]; TP =  CM[1,1]\n",
    "        \n",
    "        # Calculates tpr and fpr. \n",
    "        tpr = TP / (TP + FN)\n",
    "        fpr = FP / (FP + TN)\n",
    "    \n",
    "        # adds the tpr, fpr and threshold into the corresponding lists. \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "        thres_lish.append(threshold)\n",
    "\n",
    "    # when the for loop is end, generating a dataframe with the lists of threshold, fpr and tpr. \n",
    "    data = {'threshold':pd.Series(thres_lish), 'fpr':pd.Series(fpr_list), 'tpr':pd.Series(tpr_list)}\n",
    "    df_roc = pd.DataFrame(data)\n",
    "    \n",
    "    # descending sorting the dataframe according to the threshold column\n",
    "    df_roc.sort_values(by='threshold', ascending=False, inplace=True)\n",
    "    \n",
    "    return np.array(df_roc[\"fpr\"]), np.array(df_roc[\"tpr\"])\n",
    "\n",
    "def plotting_roc_curve(fpr, tpr, label = None): \n",
    "    plt.figure(figsize = (10, 10))\n",
    "    \n",
    "    # linewidth and fontsize\n",
    "    lw = 2\n",
    "    fontsize = 20\n",
    "    \n",
    "    # plot roc curve\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw = lw, label = label) \n",
    "    \n",
    "    # plot y = x\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw = lw, linestyle = '--')  \n",
    "    \n",
    "    # set the length of x axis and y axis. \n",
    "    plt.axis([0, 1, 0, 1.05])\n",
    "    \n",
    "    # add title, xlabel, ylabel, and legend. \n",
    "    plt.title(f'Receiver operating characteristic Curve ({label})', fontsize = fontsize)\n",
    "    plt.xlabel('False Positive Rate', fontsize = fontsize)\n",
    "    plt.ylabel('True Positive Rate', fontsize = fontsize)\n",
    "    plt.legend(loc=\"lower right\", fontsize = fontsize)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_precision_recall_curve_elements(y_label, y_prob, target_label = 1):\n",
    "    \n",
    "    # gets the target label.\n",
    "    if target_label == 0: non_target_label = 1\n",
    "    if target_label == 1: non_target_label = 0\n",
    "    \n",
    "    # converts the input arguments into arrays.\n",
    "    ar_y_label = np.array(y_label)\n",
    "    ar_y_prob = np.array(y_prob)\n",
    "    \n",
    "    # creates a list to sort the results of predicted y. \n",
    "    y_pred = list(y_prob)\n",
    "    \n",
    "    # generates list to store the tpr, fpr and threshold.\n",
    "    precision_list = []     \n",
    "    recall_list = []\n",
    "    thres_lish = []\n",
    "    \n",
    "    # using the for loop to predicte y based on the input y_prob. \n",
    "    \n",
    "    for i, prob in enumerate(ar_y_prob):\n",
    "        threshold = prob\n",
    "        for index, y_prob in enumerate(ar_y_prob):\n",
    "            if y_prob >= threshold:\n",
    "                y_pred[index] = target_label\n",
    "            else:\n",
    "                y_pred[index] = non_target_label\n",
    "        \n",
    "        # uses the function to compute the confusion matrix, and gets the TN, FP, FN, TP. \n",
    "        CM = compute_confusion_matrix(y_label, y_pred).to_numpy()           \n",
    "        TN = CM[0,0]; FP = CM[0,1]; FN = CM[1,0]; TP =  CM[1,1]\n",
    "        \n",
    "        # Calculates tpr and fpr. \n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "    \n",
    "        # adds the tpr, fpr and threshold into the corresponding lists. \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        thres_lish.append(threshold)\n",
    "\n",
    "    # when the for loop is end, generating a dataframe with the lists of threshold, fpr and tpr. \n",
    "    data = {'threshold':pd.Series(thres_lish), 'precision':pd.Series(precision_list), 'recall':pd.Series(recall_list)}\n",
    "    df_roc = pd.DataFrame(data)\n",
    "    \n",
    "    # descending sorting the dataframe according to the threshold column\n",
    "    df_roc.sort_values(by='threshold', ascending = True, inplace = True)\n",
    "        \n",
    "    return np.array(df_roc[\"precision\"]), np.array(df_roc[\"recall\"]), np.array(df_roc[\"threshold\"]) \n",
    "\n",
    "def calculate_auc(fpr_x_axis, tpr_y_axis):\n",
    "    \n",
    "    # Trapezoidal numerical integration \n",
    "    auc = np.trapz(tpr_y_axis, fpr_x_axis)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = generate_ROC_elements(y_test, y_pre)\n",
    "plotting_roc_curve(fpr, tpr, \"test data\")\n",
    "print('\\n\\nAUC :',calculate_auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
