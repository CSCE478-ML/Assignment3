{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zedin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zedin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-adc1e0af2297>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", names = [\"Spam\", \"Text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"\\nNo. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = df['Spam'].map(lambda Spam:0 if Spam == 'ham' else 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"length\" the stores the length of the text on each row\n",
    "df['length'] = df['Text'].map(lambda text: len(text))\n",
    "\n",
    "df.groupby('Spam').length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['text_lemmatized'] = df['Text'].map(lambda text: ' '.join(lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text.lower())))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df.Spam.value_counts()\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Spam', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a length threshold to visualize the distribution of length per class\n",
    "\n",
    "emails_subset = df[df.length < 1000]\n",
    "emails_subset.hist(column='length', by='Spam', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text_lemmatized\"]\n",
    "\n",
    "y = df['Class'] # 1D targer vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect = CountVectorizer(lowercase=True, stop_words='english',binary = False)\n",
    "count_vect = CountVectorizer(lowercase=True, stop_words='english',binary = True)\n",
    "X_counts = count_vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIts Index rather than count\")\n",
    "count_vect.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X = np.array(X_counts.toarray())\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Y_true, Y_pred):\n",
    "    E = np.array(Y_true).reshape(-1,1) - np.array(Y_pred).reshape(-1,1)\n",
    "    mse = 1/np.array(Y_true).shape[0] * (E.T.dot(E))\n",
    "    return mse[(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multivariate_NB:\n",
    "    def __init_(self,alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "        self.pie_1 = None\n",
    "        self.pie_0 = None\n",
    "        self.theta_jc_1 = None\n",
    "        self.theta_jc_0 = None\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        pie_num = (Y == 1).astype(int).sum() + 1 \n",
    "        pie_denum = len(set(Y)) + len(Y)\n",
    "        self.pie_0 = pie_num / pie_denum       \n",
    "        self.pie_1 = 1 - self.pie_0\n",
    "        \n",
    "        \n",
    "        N_jc_0 =  X[Y == 0].sum(axis=0) \n",
    "        N_c_0 = X[Y==0].shape[0]\n",
    "        self.theta_jc_0 = (N_jc_0 + 1) / (2 + N_c_0)\n",
    "        \n",
    "        N_jc_1 =  X[Y == 1].sum(axis=0) \n",
    "        N_c_1 = X[Y==1].shape[0]\n",
    "        self.theta_jc_1 = (N_jc_1 + 1) / (2 + N_c_1)               \n",
    "        \n",
    "#         ###without laplace Smoothing\n",
    "#         self.pie_1 = Y.mean()\n",
    "#         self.pie_0 = 1 - self.pie_1\n",
    "#         self.theta_jc_0 = X[Y == 0].mean(axis=0)\n",
    "#         self.theta_jc_1 = X[Y == 1].mean(axis=0)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        log_prob = self.predict_log_proba(X)\n",
    "        \n",
    "        # print((log_prob[:,0] <= log_prob[:,1]).astype(int))\n",
    "        return np.array((log_prob[:,0] <= log_prob[:,1]).astype(int))\n",
    "        \n",
    "    def predict_log_proba(self, X):\n",
    "        a = self.theta_jc_1[:,None]\n",
    "        b = self.theta_jc_0[:,None]\n",
    "        log_prob_1 = np.log(self.pie_1) + np.log(np.where(X.T*a != 0,a, 1-a )).sum(axis=0).T\n",
    "        log_prob_1 = log_prob_1[:,None]\n",
    "        log_prob_0 = np.log(self.pie_0) + np.log(np.where(X.T*b != 0,b, 1-b )).sum(axis=0).T\n",
    "        log_prob_0 = log_prob_0[:,None]\n",
    "        return np.concatenate((log_prob_0,log_prob_1),axis = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multinomial_NB:\n",
    "    def __init(self,alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.pie_1 = (Y.sum() + 1 ) / (len(set(Y)) + len(Y))\n",
    "\n",
    "        self.theta_jc = np.zeros((2, X.shape[1]))\n",
    "        ham_doc = X[Y == 0]\n",
    "        self.theta_jc[0] = (ham_doc.sum(axis=0) + 1) / (np.einsum('ij->',ham_doc) + X.shape[1])\n",
    "\n",
    "        spam_doc = X[Y == 1]\n",
    "        self.theta_jc[1] = (spam_doc.sum(axis=0)+1) / (np.einsum('ij->',spam_doc)+X.shape[1])\n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return np.sum(X[:,None] * np.log(self.theta_jc), axis=-1) + np.log([1-self.pie_1,self.pie_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,y):\n",
    "    x,y = np.array(x),np.array(y)\n",
    "    pred = (x == y).astype(np.int)\n",
    "    return pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multivariate_NB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pre = model.predict(X_test)\n",
    "print(y_pre)\n",
    "mse(y_test, y_pre)\n",
    "accuracy(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pre_sk = clf.predict(X_test)\n",
    "print(y_pre_sk)\n",
    "mse(y_test, y_pre_sk)\n",
    "accuracy(y_test, y_pre_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multinomial_NB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pre = model.predict(X_test)\n",
    "y_pre\n",
    "mse(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pre_sk = clf.predict(X_test)\n",
    "y_pre_sk\n",
    "mse(y_test, y_pre_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
